# Intuitives DAW - Complete Documentation

<p align="center">
  <strong>üéµ Revolutionary Rule-Free Digital Audio Workstation üéµ</strong>
</p>

<p align="center">
  <em>"Does this sound cool?" - The only rule.</em>
</p>

---

## Table of Contents

1. [Introduction](#introduction)
2. [Philosophy & Vision](#philosophy--vision)
3. [Key Features](#key-features)
4. [Architecture Overview](#architecture-overview)
5. [Installation & Setup](#installation--setup)
6. [Getting Started](#getting-started)
7. [Core Concepts](#core-concepts)
8. [AI Integration](#ai-integration)
9. [Plugin System](#plugin-system)
10. [Advanced Usage](#advanced-usage)
11. [Contributing](#contributing)
12. [License](#license)

---

## Introduction

**Intuitives DAW** is an open-source, experimental Digital Audio Workstation that revolutionizes music creation by removing traditional barriers and embracing intuition, experimentation, and creativity. Unlike conventional DAWs that enforce music theory rules, scales, and technical constraints, Intuitives empowers everyone to create music through visual, progressive, and intuitive approaches.

### What Makes Intuitives Different?

- **üé≤ Experiment-First Design** - Try ideas instantly without theory knowledge
- **üö´ No Learning Curve** - If you can play, you can create
- **üé® Visual & Intuitive** - See your sound, shape it visually
- **ü§ñ AI Optional** - Use AI assistance when you want it, not when forced
- **üå± Sustainability Focus** - Locally-run models preferred, no cloud dependency
- **üîå Plugin-Open Architecture** - Extend and customize everything
- **üìñ 100% Open Source** - Free forever, community-driven

---

## Philosophy & Vision

### The Core Philosophy

Intuitives is built on the revolutionary principle that **music creation should be accessible to everyone**, regardless of their formal musical training. Traditional music production tools impose technical and theoretical barriers that prevent creative expression. Intuitives removes these barriers entirely.

#### Music for Everyone

**Traditional Approach:**
- Requires understanding of scales, keys, time signatures
- Forces conformity to music theory rules
- Demands technical knowledge of audio engineering
- Creates steep learning curves
- Limits experimentation

**Intuitives Approach:**
- **No Rules** - Music theory is optional, not required
- **Visual Feedback** - See your sound through colors, shapes, and patterns
- **Instant Experimentation** - Try anything, hear results immediately
- **Progressive Learning** - Grow naturally through play and discovery
- **Creative Freedom** - Express yourself without constraints

#### Bypassing Music Theory Traditions

Intuitives fundamentally reimagines how we interact with music:

1. **Multimedia Input as Music Seeds**
   - Drop an image ‚Üí Generate harmonies from colors
   - Type text ‚Üí Convert to melodies
   - Draw shapes ‚Üí Create rhythmic patterns
   - Use webcam gestures ‚Üí Control sound in real-time

2. **Visual Sound Manipulation**
   - See waveforms as you create them
   - Pull and shape audio visually
   - Color-coded notes (Chromasynesthesia)
   - Geometric fractals react to your music

3. **AI as Creative Partner**
   - Not a "generate and forget" tool
   - Real-time jamming and harmonization
   - Suggestions, not dictations
   - Learn your style and adapt

4. **Generative & Algorithmic Tools**
   - Markov chains for melody evolution
   - Genetic algorithms for sound design
   - Cellular automata for rhythmic patterns
   - L-systems for fractal compositions

### The Experiment-First Paradigm

Every feature in Intuitives is designed for **immediate experimentation**:

- **Quick Iteration** - Hear your changes instantly
- **Undo Anything** - Experiment without fear
- **Random Inspiration** - Generate starting points
- **Exploration Tools** - Discover new sonic territories

### Complete Yet Accessible

While Intuitives is **complete** with professional-grade features, it maintains **zero learning curve**:

- **Professional Audio Engine** - 48kHz/24-bit quality
- **40+ Native DSP Features** - Studio-quality effects
- **Unlimited Tracks** - No artificial limitations
- **Full Automation** - Professional workflow support
- **Export Anywhere** - Standard audio formats

Yet beginners can create music **within minutes** of opening the app.

---

## Key Features

### üéπ Native Audio Engine (40+ Features)

#### Oscillators (7 Types)
- **Quantum Oscillator** - Multi-waveform with real-time morphing
- **Chaos Oscillator** - Lorenz attractor-based synthesis
- **Wavetable Oscillator** - Band-limited with interpolation
- **FM Synthesis** - 6-operator with configurable algorithms
- **Additive Synthesis** - 64 partials with spectral shaping
- **Noise Generator** - White, Pink, Brown, Velvet, and more
- **Fractal Oscillator** - Mandelbrot/Julia-derived harmonics

#### Effects (12 Types)
- **Filters** - State Variable, Moog Ladder, Formant
- **Time-Based** - Multi-tap Delay, Schroeder Reverb
- **Modulation** - Chorus (8 voices), Phaser (12-stage)
- **Dynamics** - Compressor/Limiter with sidechain
- **Distortion** - Waveshaper with 8 algorithms
- **Creative** - Bitcrusher, Granular Synthesis

#### Generative Tools (8 Types)
- **Markov Melody** - Probabilistic note generation
- **Genetic Algorithm** - Evolving melodies
- **Cellular Automata** - Rule-based rhythm patterns
- **L-System Generator** - Fractal compositions
- **Brownian Motion** - Random walk melodies
- **Stochastic Sequencer** - Probability-based triggers
- **Chord Progression** - Functional harmony
- **Spectral Processing** - Freeze, blur, shift, robotize

#### Visual Tools (6 Types)
- **Waveform Scope** - Real-time oscilloscope
- **Spectrum Analyzer** - FFT with peak hold
- **Phase Correlator** - Stereo field visualization
- **Level Meters** - True peak with clip detection
- **Chromasynesthesia** - Pitch-to-color mapping
- **Fluid Simulation Bridge** - Physics-reactive visuals

### ü§ñ AI Integration (Optional)

#### Why AI is Optional

**Philosophy:** AI should enhance creativity, not replace it. You remain in full control.

- ‚úÖ **Works Offline** - Run locally without internet
- ‚úÖ **Your Choice** - Enable only features you want
- ‚úÖ **Privacy First** - No data sent to cloud by default
- ‚úÖ **Multiple Backends** - OpenAI, Anthropic, or Local models

#### AI Features Available

1. **Chord Progression Suggestions** - Context-aware harmony ideas
2. **Melody Generation** - Complementary melodic lines
3. **Arrangement Analysis** - Feedback on song structure
4. **Mixing Advice** - EQ and effect recommendations
5. **Mastering Suggestions** - Final polish guidance
6. **Interactive Chat** - Ask production questions

#### Local Model Support (Sustainable AI)

**Recommended Local Models:**
- **Magenta** - Google's music AI (fully offline)
- **MusicGen** - Meta's AudioCraft (local inference)
- **Basic Pitch** - Spotify's audio-to-MIDI (offline)
- **Spleeter** - Deezer's stem separation (local)

**Why Local Models?**
- üå± **Environmental Sustainability** - Lower carbon footprint
- üîí **Privacy & Security** - Data stays on your machine
- ‚ö° **Lower Latency** - No network delays
- üí∞ **Cost-Free** - No API fees or subscriptions
- üîå **Offline Capable** - Work anywhere, anytime

### üîå Plugin-Open Architecture

#### Complete Extensibility

Every component is designed to be extended:

```
‚úÖ Custom Audio Effects
‚úÖ Custom MIDI Processors
‚úÖ Custom Generators
‚úÖ Custom Visualizers
‚úÖ Custom AI Models
‚úÖ Custom Input Sources
‚úÖ Custom Export Formats
```

#### Plugin API

Simple, well-documented Python API:

```python
from intuitive_daw.audio.processor import AudioEffect
import numpy as np

class MyEffect(AudioEffect):
    def __init__(self, param=0.5):
        super().__init__("My Effect")
        self.param = param
    
    def _process_impl(self, audio: np.ndarray) -> np.ndarray:
        # Your processing here
        return audio * self.param
```

#### Plugin Discovery

- Drop plugins in `plugins/` or `user_plugins/`
- Auto-discovered on startup
- Hot-reload during development
- Share plugins with the community

### üåç Open Source Commitment

#### Licensing

- **Native Engine:** MIT License (permissive)
- **Stargate Components:** GPLv3 (copyleft)
- **Python Tools:** MIT License (permissive)
- **Documentation:** CC BY-SA 4.0 (share-alike)

#### Community-Driven

- üìù **Open Development** - All decisions are public
- üêõ **Public Issue Tracking** - Transparent roadmap
- üîÄ **Pull Requests Welcome** - All contributions valued
- üí¨ **Active Community** - Discord, Forums, GitHub Discussions
- üìñ **Open Documentation** - Everyone can contribute

#### No Vendor Lock-In

- Standard file formats (WAV, MIDI, JSON)
- Open plugin API
- Portable projects
- Export to any DAW

---

## Architecture Overview

### Three-Layer Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Frontend Layer (Optional)        ‚îÇ
‚îÇ  React UI / Native macOS App / CLI      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ REST API / WebSocket
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Python Application Layer         ‚îÇ
‚îÇ  AI Integration / Plugin System /       ‚îÇ
‚îÇ  Project Management / MIDI Processing   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ FFI / C Bindings
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Native Audio Engine (C/C++)      ‚îÇ
‚îÇ  Real-time DSP / Oscillators /          ‚îÇ
‚îÇ  Effects / Low-latency Processing       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Modular Components

#### Core Engine (`native/`)
- **Language:** C17/C++17
- **Purpose:** Real-time audio processing
- **Features:** SIMD optimization, lock-free algorithms
- **Platforms:** macOS, Linux, Windows, WebAssembly

#### Python Layer (`src/intuitive_daw/`)
- **Language:** Python 3.9+
- **Purpose:** High-level logic, AI, plugins
- **Components:**
  - `core/` - Project management, tracks
  - `audio/` - Python audio effects
  - `midi/` - MIDI processing
  - `ai/` - AI assistant integration
  - `api/` - REST API server
  - `plugins/` - Plugin system

#### Frontend (`frontend/`)
- **Framework:** React 18
- **Purpose:** Web-based user interface
- **Features:** Real-time waveforms, WebSocket sync

### Data Flow

```
User Input
    ‚Üì
Multimedia Converter (Image/Text/Gesture ‚Üí Audio/MIDI)
    ‚Üì
Generative Engine (Optional AI Enhancement)
    ‚Üì
Audio Engine (Real-time Processing)
    ‚Üì
Visual Feedback (Waveforms, Spectrums, Colors)
    ‚Üì
Export (WAV, MP3, MIDI, Project)
```

---

## Installation & Setup

### Quick Start (5 Minutes)

#### Prerequisites

- **Python 3.9+** - `python --version`
- **pip** - `pip --version`
- **(Optional) Node.js 16+** - For web UI

#### Step 1: Clone Repository

```bash
git clone https://github.com/mitchlabeetch/Intuitive_Music.git
cd Intuitive_Music
```

#### Step 2: Install Python Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
source venv/bin/activate  # macOS/Linux
# OR
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Install Intuitives DAW
pip install -e .
```

#### Step 3: Initialize

```bash
intuitive-daw init
```

#### Step 4: Run!

```bash
# Option A: Start server (web UI)
intuitive-daw serve

# Option B: Run native app (macOS)
cd native/intuitives_daw
./build.sh
open build/IntuitivesDAW.app

# Option C: Use Python directly
python native/intuitives_daw/intuitives.py
```

### Platform-Specific Instructions

#### macOS

```bash
# Install build tools
xcode-select --install
brew install cmake

# Build native app
cd native/intuitives_daw
./build.sh --gui
```

#### Linux

```bash
# Install dependencies
sudo apt install build-essential cmake libasound2-dev

# Build native app
cd native/intuitives_daw
./build.sh
```

#### Windows

```bash
# Install Visual Studio 2019+ with C++ tools
# Install CMake

# Build native app
cd native/intuitives_daw
mkdir build && cd build
cmake .. -G "Visual Studio 16 2019"
cmake --build . --config Release
```

### Optional AI Features

#### Local Models (Recommended)

```bash
# Install Magenta (Google's music AI)
pip install magenta

# Install AudioCraft (Meta's text-to-music)
pip install audiocraft

# Install Basic Pitch (Spotify's audio-to-MIDI)
pip install basic-pitch

# Install Spleeter (Deezer's stem separation)
pip install spleeter
```

#### Cloud API (Optional)

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add API keys (optional)
# OPENAI_API_KEY=your-key-here
# ANTHROPIC_API_KEY=your-key-here
```

**Note:** Cloud APIs are **completely optional**. All core functionality works without them.

---

## Getting Started

### Your First 5 Minutes

#### 1. Create a Project

```bash
intuitive-daw create "My First Song"
```

Or use the GUI:
- Click "New Project"
- Enter a name
- Press Create

#### 2. Generate Your First Melody

**Option A: From Text**
- Type any text: "Hello World"
- Click "Text to Melody"
- Listen to your text as music!

**Option B: From Color**
- Pick a color: #FF5C5C (red)
- Click "Color to Harmony"
- Hear the color as a chord!

**Option C: Random Generation**
- Click "Markov Melody"
- Get a unique melody every time
- Tweak parameters and regenerate

#### 3. Add Effects

- Select your track
- Add effects: Reverb, Delay, Distortion
- Adjust parameters in real-time
- Hear changes immediately

#### 4. Visualize Your Sound

- Enable Waveform display
- Enable Spectrum Analyzer
- Watch colors change (Chromasynesthesia)
- See your music as art

#### 5. Export

```bash
intuitive-daw export ./projects/my-first-song output.wav
```

**Congratulations!** You've created music without learning a single music theory concept.

### Tutorial: Image to Music

Let's create music from an image:

```python
from intuitive_daw import Project
from intuitive_daw.ai.assistant import AIAssistant
from PIL import Image
import numpy as np

# Load an image
image = Image.open("sunset.jpg")
pixels = np.array(image)

# Extract colors from image
# Each row becomes a harmonic
colors = pixels[::10, ::10, :3]  # Sample every 10th pixel

# Convert colors to notes
# Red ‚Üí C, Green ‚Üí E, Blue ‚Üí G
notes = []
for color in colors.reshape(-1, 3):
    r, g, b = color / 255.0
    note = int(60 + r * 12 + g * 7 + b * 5)  # Map to MIDI range
    notes.append(note)

# Create MIDI from notes
from intuitive_daw.midi.processor import MIDIClip
clip = MIDIClip("Image Melody")
time = 0.0
for note in notes[:32]:  # First 32 notes
    clip.add_note(pitch=note, velocity=80, start=time, duration=0.25)
    time += 0.25

# Add to project and render
project = Project("Sunset Song")
track = project.add_midi_track("Image Track")
track.add_clip(clip)
project.save()
```

**Result:** Your sunset image is now a melody!

### Tutorial: Gesture Control (Webcam)

Control music with hand gestures:

```python
import cv2
import mediapipe as mp
from intuitive_daw.core.engine import AudioEngine

# Initialize webcam and hand tracking
cap = cv2.VideoCapture(0)
hands = mp.solutions.hands.Hands()
engine = AudioEngine()
engine.initialize()

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Detect hand
    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Get index finger position
            index_finger = hand_landmarks.landmark[8]
            x, y = index_finger.x, index_finger.y
            
            # Map to audio parameters
            filter_cutoff = 200 + x * 4000  # 200Hz to 4200Hz
            reverb_amount = y  # 0.0 to 1.0
            
            # Update audio engine parameters
            engine.set_parameter("filter_cutoff", filter_cutoff)
            engine.set_parameter("reverb_mix", reverb_amount)
    
    cv2.imshow('Gesture Control', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
```

**Result:** Move your hand to control the music in real-time!

---

## Core Concepts

### Tracks

Tracks are containers for audio or MIDI:

```python
from intuitive_daw.core.track import AudioTrack, MIDITrack

# Audio track for recordings
audio_track = AudioTrack("Vocals")
audio_track.volume = 0.8
audio_track.pan = 0.0  # Center

# MIDI track for instruments
midi_track = MIDITrack("Piano")
midi_track.add_effect(reverb)
```

### Effects Chain

Stack effects in any order:

```python
from intuitive_daw.audio.processor import (
    EQEffect, CompressorEffect, ReverbEffect
)

track.add_effect(EQEffect(low_gain=3.0))
track.add_effect(CompressorEffect(threshold_db=-20))
track.add_effect(ReverbEffect(room_size=0.6))
```

### Automation

Automate any parameter over time:

```python
from intuitive_daw.core.automation import Automation

# Create volume automation
auto = Automation("Volume")
auto.add_point(time=0.0, value=0.0)    # Start silent
auto.add_point(time=2.0, value=1.0)    # Fade in to full
auto.add_point(time=8.0, value=1.0)    # Stay full
auto.add_point(time=10.0, value=0.0)   # Fade out

track.add_automation(auto)
```

### Generators

Use algorithmic generators:

```python
from intuitive_daw.midi.processor import MIDIProcessor

# Markov chain melody
markov_melody = MIDIProcessor.generate_markov_melody(
    seed_notes=[60, 64, 67],  # C major triad
    length=32,
    temperature=0.7
)

# Genetic algorithm evolution
evolved_melody = MIDIProcessor.evolve_melody(
    initial_melody=markov_melody,
    generations=10,
    fitness_function=lambda m: evaluate_melody(m)
)

# Cellular automata rhythm
rhythm = MIDIProcessor.cellular_automaton(
    rule=30,  # Rule 30 is chaotic
    steps=16,
    initial_state=[1, 0, 0, 0]
)
```

### Chromasynesthesia

Every note has a color:

```python
from intuitive_daw.utils.chromasynesthesia import note_to_color

# MIDI note to color
color = note_to_color(60)  # C4 ‚Üí "#ff5c5c" (red)
color = note_to_color(64)  # E4 ‚Üí "#e8ff4c" (yellow)
color = note_to_color(67)  # G4 ‚Üí "#4cffff" (cyan)

# Color to chord
from intuitive_daw.utils.chromasynesthesia import color_to_chord
chord = color_to_chord("#ff5c5c")  # Red ‚Üí C major chord
```

---

## AI Integration

### Philosophy

AI in Intuitives is a **creative partner**, not a replacement for creativity:

- **Suggestive, Not Prescriptive** - Offers ideas, you decide
- **Context-Aware** - Understands your musical context
- **Iterative** - Works with you, not instead of you
- **Optional** - Disable entirely if you prefer

### Local vs Cloud

#### Local Models (Recommended)

**Advantages:**
- ‚úÖ Privacy - Data never leaves your computer
- ‚úÖ Offline - No internet required
- ‚úÖ Free - No API costs
- ‚úÖ Low latency - Instant responses
- ‚úÖ Sustainable - Lower energy use

**Setup:**
```bash
pip install magenta audiocraft basic-pitch spleeter
```

**Usage:**
```python
from intuitive_daw.ai.local_models import MagentaMelodyRNN

# Use local Magenta model
ai = MagentaMelodyRNN()
melody = ai.generate_melody(
    primer_notes=[60, 64, 67],
    length=32
)
```

#### Cloud APIs (Optional)

**When to use:**
- More sophisticated language understanding
- Advanced composition analysis
- Natural conversation

**Setup:**
```bash
# In .env file
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

**Usage:**
```python
from intuitive_daw.ai.assistant import AIAssistant

ai = AIAssistant(provider="openai")  # or "anthropic"
response = ai.chat("How can I make my bass sound punchier?")
```

### AI Features

#### Chord Progression Suggestions

```python
response = ai.suggest_chords(
    key="C major",
    style="pop",
    current_chords=["C", "Am"]
)
# Returns: ["F", "G"] to complete the progression
```

#### Melody Generation

```python
melody = ai.generate_melody(
    key="C major",
    chord_progression=["C", "Am", "F", "G"],
    style="uplifting"
)
# Returns: MIDI notes that fit the chords
```

#### Mixing Advice

```python
advice = ai.mixing_advice(
    track_name="Vocals",
    issues=["muddy", "harsh sibilance"],
    context="pop ballad"
)
# Returns: Specific EQ and compression suggestions
```

#### Arrangement Analysis

```python
analysis = ai.analyze_arrangement(project)
# Returns: Feedback on structure, dynamics, balance
```

### Custom AI Models

Integrate your own models:

```python
from intuitive_daw.ai.base import AIModel

class MyCustomModel(AIModel):
    def __init__(self):
        super().__init__("My Custom Model")
    
    def generate_melody(self, **kwargs):
        # Your model logic here
        return melody_notes
    
    def generate_chords(self, **kwargs):
        # Your model logic here
        return chord_progression

# Register your model
from intuitive_daw.ai import register_model
register_model(MyCustomModel())
```

---

## Plugin System

### Why Plugins?

Extend Intuitives with custom functionality:

- üéöÔ∏è **Audio Effects** - Create unique sound processors
- üéπ **Instruments** - Build virtual synthesizers
- üéµ **MIDI Processors** - Custom note manipulation
- üé® **Visualizers** - Custom visual feedback
- ü§ñ **AI Models** - Integrate new AI capabilities
- üìÅ **Import/Export** - Support new file formats

### Plugin Architecture

```
plugins/
‚îú‚îÄ‚îÄ my_effect/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ effect.py
‚îÇ   ‚îú‚îÄ‚îÄ manifest.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ my_instrument/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ synth.py
    ‚îú‚îÄ‚îÄ manifest.json
    ‚îî‚îÄ‚îÄ README.md
```

### Creating an Audio Effect Plugin

#### Step 1: Create Plugin Structure

```bash
mkdir plugins/my_reverb
cd plugins/my_reverb
touch __init__.py effect.py manifest.json
```

#### Step 2: Write manifest.json

```json
{
  "name": "My Reverb",
  "version": "1.0.0",
  "author": "Your Name",
  "description": "Custom reverb effect",
  "type": "audio_effect",
  "entry_point": "effect.MyReverb",
  "parameters": {
    "room_size": {
      "type": "float",
      "min": 0.0,
      "max": 1.0,
      "default": 0.5
    },
    "damping": {
      "type": "float",
      "min": 0.0,
      "max": 1.0,
      "default": 0.5
    }
  }
}
```

#### Step 3: Implement Effect

```python
# effect.py
from intuitive_daw.audio.processor import AudioEffect
import numpy as np

class MyReverb(AudioEffect):
    """Custom reverb implementation"""
    
    def __init__(self, room_size=0.5, damping=0.5):
        super().__init__("My Reverb")
        self.room_size = room_size
        self.damping = damping
        self.buffer = None
    
    def _process_impl(self, audio: np.ndarray) -> np.ndarray:
        """Process audio with reverb"""
        # Initialize buffer on first call
        if self.buffer is None:
            buffer_size = int(self.sample_rate * self.room_size)
            self.buffer = np.zeros((buffer_size, audio.shape[1]))
        
        # Simple reverb algorithm
        output = np.copy(audio)
        for i in range(len(audio)):
            # Add delayed signal
            delay_sample = self.buffer[0]
            output[i] += delay_sample * (1 - self.damping)
            
            # Update buffer
            self.buffer = np.roll(self.buffer, -1, axis=0)
            self.buffer[-1] = audio[i]
        
        return output
    
    def set_parameter(self, name: str, value: float):
        """Update parameter"""
        if name == "room_size":
            self.room_size = value
            self.buffer = None  # Reset buffer
        elif name == "damping":
            self.damping = value
```

#### Step 4: Test Your Plugin

```python
# test_plugin.py
from plugins.my_reverb.effect import MyReverb
import numpy as np

# Create test signal
sample_rate = 48000
duration = 1.0
samples = int(sample_rate * duration)
test_signal = np.random.randn(samples, 2) * 0.1

# Apply reverb
reverb = MyReverb(room_size=0.7, damping=0.3)
reverb.sample_rate = sample_rate
output = reverb.process(test_signal)

# Verify output
assert output.shape == test_signal.shape
print("Plugin test passed!")
```

### Creating a MIDI Processor Plugin

```python
# plugins/arpeggiator/processor.py
from intuitive_daw.midi.processor import MIDIClip

class ArpeggiatorPlugin:
    """Custom arpeggiator plugin"""
    
    def __init__(self, pattern="up", rate=0.25):
        self.pattern = pattern  # up, down, up-down, random
        self.rate = rate  # Note duration
    
    def process(self, clip: MIDIClip) -> MIDIClip:
        """Convert chords to arpeggios"""
        output = MIDIClip(f"{clip.name} - Arpeggiated")
        
        # Group notes by start time (find chords)
        chords = {}
        for note in clip.notes:
            start = round(note.start, 2)
            if start not in chords:
                chords[start] = []
            chords[start].append(note)
        
        # Arpeggiate each chord
        for start_time, chord_notes in sorted(chords.items()):
            # Sort notes based on pattern
            if self.pattern == "up":
                sorted_notes = sorted(chord_notes, key=lambda n: n.pitch)
            elif self.pattern == "down":
                sorted_notes = sorted(chord_notes, key=lambda n: n.pitch, reverse=True)
            else:  # random
                sorted_notes = chord_notes.copy()
                np.random.shuffle(sorted_notes)
            
            # Create arpeggio
            time = start_time
            for note in sorted_notes:
                output.add_note(
                    pitch=note.pitch,
                    velocity=note.velocity,
                    start=time,
                    duration=self.rate
                )
                time += self.rate
        
        return output
```

### Plugin Distribution

#### Sharing Your Plugin

1. **Create README.md** with usage instructions
2. **Test thoroughly** on different systems
3. **Package as ZIP** or publish to GitHub
4. **Submit to Plugin Directory** (community maintained)

#### Installing Community Plugins

```bash
# From URL
intuitive-daw plugin install https://github.com/user/plugin.git

# From local file
intuitive-daw plugin install ./my-plugin.zip

# Browse plugin directory
intuitive-daw plugin browse
```

---

## Advanced Usage

### Scripting & Automation

Automate repetitive tasks:

```python
# scripts/generate_variations.py
from intuitive_daw import Project
from intuitive_daw.midi.processor import MIDIProcessor

# Load base melody
project = Project.load("./projects/base-song")
melody = project.tracks[0].clips[0]

# Generate 10 variations
for i in range(10):
    variation = MIDIProcessor.markov_variation(
        melody,
        temperature=0.5 + i * 0.05
    )
    
    # Save as new project
    new_project = Project(f"Variation {i+1}")
    new_project.add_track("Melody").add_clip(variation)
    new_project.save(f"./projects/variation-{i+1}")
```

### Batch Processing

Process multiple files:

```bash
# Batch export all projects
for project in ./projects/*; do
    intuitive-daw export "$project" "./exports/$(basename $project).wav"
done
```

### Live Coding

Use Intuitives as a live coding environment:

```python
# live_session.py
from intuitive_daw.core.engine import AudioEngine
from intuitive_daw.midi.processor import MIDIClip
import time

engine = AudioEngine()
engine.initialize()
engine.start_playback()

# Live loop
while True:
    # Generate new pattern
    clip = MIDIClip("Live Pattern")
    for i in range(16):
        note = 60 + np.random.choice([0, 2, 4, 7])  # Pentatonic
        clip.add_note(note, 80, i * 0.25, 0.25)
    
    # Play immediately
    engine.play_clip(clip)
    
    # Wait for next loop
    time.sleep(4.0)  # 16 beats at 120 BPM
```

### Integration with Other Tools

#### Export to Other DAWs

```python
# Export MIDI for use in Ableton, FL Studio, etc.
from intuitive_daw.export import export_midi

export_midi(project, "output.mid", format="type1")
```

#### Import from Other Sources

```python
# Import MIDI from other DAWs
from intuitive_daw.import import import_midi

project = Project("Imported Song")
midi_data = import_midi("external.mid")
for track_data in midi_data:
    track = project.add_midi_track(track_data.name)
    track.add_clip(track_data.clip)
```

### Performance Optimization

#### Enable GPU Acceleration (if available)

```yaml
# config.yaml
processing:
  use_gpu: true
  gpu_device: 0
```

#### Adjust Buffer Size

```yaml
# config.yaml
audio:
  buffer_size: 256  # Lower = less latency, higher CPU
  # Try: 128 (low latency), 512 (balanced), 1024 (safe)
```

#### Parallel Processing

```yaml
# config.yaml
processing:
  parallel_processing: true
  worker_threads: 8  # Number of CPU cores
```

---

## Contributing

### How to Contribute

We welcome all contributions! Here's how:

#### 1. Fork & Clone

```bash
git clone https://github.com/YOUR_USERNAME/Intuitive_Music.git
cd Intuitive_Music
git remote add upstream https://github.com/mitchlabeetch/Intuitive_Music.git
```

#### 2. Create a Branch

```bash
git checkout -b feature/my-new-feature
# or
git checkout -b fix/bug-description
```

#### 3. Make Changes

- Follow existing code style
- Add tests for new features
- Update documentation
- Keep commits focused and descriptive

#### 4. Test

```bash
# Run tests
pytest tests/

# Run linters
black src/
flake8 src/

# Test native build
cd native/intuitives_daw
./build.sh
```

#### 5. Submit Pull Request

- Push to your fork
- Open PR on main repository
- Describe your changes
- Link related issues

### What to Contribute

#### Code
- üêõ **Bug Fixes** - Fix issues
- ‚ú® **New Features** - Add functionality
- ‚ö° **Performance** - Optimize code
- üé® **UI/UX** - Improve interface

#### Documentation
- üìù **Tutorials** - Write guides
- üìñ **API Docs** - Document functions
- üåç **Translations** - Localize content
- üé• **Videos** - Create screencasts

#### Community
- üí¨ **Support** - Help other users
- üéµ **Demos** - Share your music
- üîå **Plugins** - Create extensions
- üêõ **Bug Reports** - Report issues

### Code of Conduct

Be respectful, inclusive, and constructive. We're all here to make music more accessible.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run in development mode
FLASK_ENV=development intuitive-daw serve
```

---

## License

### Open Source Commitment

Intuitives is **free forever** and will always remain open source.

### License Terms

- **Native Engine:** [MIT License](LICENSE-MIT) - Use anywhere, including commercial
- **Stargate Components:** [GPLv3](LICENSE-GPL) - Copyleft, derivatives must be open source
- **Python Tools:** [MIT License](LICENSE-MIT) - Permissive
- **Documentation:** [CC BY-SA 4.0](LICENSE-CC) - Share alike

### What This Means

‚úÖ **You CAN:**
- Use Intuitives for commercial projects
- Modify the code
- Distribute modified versions
- Use in proprietary software (MIT parts)
- Sell music created with Intuitives

‚ùå **You MUST:**
- Include license notices
- State changes made (GPLv3 parts)
- Make source available (GPLv3 parts)

### Third-Party Licenses

See [THIRD_PARTY_LICENSES.md](THIRD_PARTY_LICENSES.md) for dependencies.

---

## Support & Community

### Get Help

- üìñ **Documentation:** You're reading it!
- üí¨ **Discord:** [Join our community](https://discord.gg/intuitives)
- üêõ **Issues:** [GitHub Issues](https://github.com/mitchlabeetch/Intuitive_Music/issues)
- üìß **Email:** support@intuitivesdaw.com

### Stay Updated

- ‚≠ê **Star on GitHub** - Show your support
- üëÅÔ∏è **Watch Repository** - Get notified of updates
- üê¶ **Follow on Twitter** - @IntuitivesDe AW
- üì∫ **YouTube Channel** - Tutorials and demos

### Share Your Music

Created something with Intuitives? Share it!

- Tag **#IntuitivesDAW** on social media
- Submit to our [Community Showcase](https://intuitivesdaw.com/showcase)
- Join monthly [Remix Competitions](https://intuitivesdaw.com/remix)

---

## Frequently Asked Questions

### General

**Q: Do I need to know music theory?**
A: No! That's the whole point. Intuitives is designed for creation without theory knowledge.

**Q: Is it really free?**
A: Yes, 100% free and open source. Forever.

**Q: Can I use it commercially?**
A: Yes, you own all music you create with Intuitives.

### Technical

**Q: What operating systems are supported?**
A: macOS, Linux, Windows. WebAssembly version coming soon.

**Q: Does it work offline?**
A: Yes, completely. Cloud APIs are optional.

**Q: Can I use VST plugins?**
A: VST support is on the roadmap. Currently uses native effects and custom plugins.

### AI Features

**Q: Do I have to use AI features?**
A: No, they're completely optional. Disable them in settings.

**Q: Do you send my data to cloud services?**
A: Only if you enable cloud AI APIs. Local models keep everything on your computer.

**Q: Which AI provider is recommended?**
A: For privacy and sustainability, use local models (Magenta, MusicGen). For advanced features, OpenAI or Anthropic.

### Contributing

**Q: I'm not a programmer, can I still help?**
A: Absolutely! Write docs, create tutorials, report bugs, share feedback, or make music!

**Q: How long until my PR is reviewed?**
A: Usually within a week. We're a small team but active.

---

## Acknowledgments

### Built With

- **Stargate DAW** - Professional audio engine foundation
- **Magenta** - Google's music AI
- **AudioCraft** - Meta's text-to-music
- **Librosa** - Audio analysis
- **Flask** - Web framework
- **React** - UI framework
- **NumPy/SciPy** - Scientific computing

### Inspired By

- **SuperCollider** - Live coding music
- **Sonic Pi** - Code-based music creation
- **VCV Rack** - Modular synthesis
- **Hydra** - Visual live coding

### Contributors

Thank you to all our contributors! See [CONTRIBUTORS.md](CONTRIBUTORS.md) for the full list.

---

## Roadmap

### Coming Soon

- [ ] **VST/AU Plugin Support** - Use your favorite plugins
- [ ] **Mobile App** - Create on phone/tablet
- [ ] **WebAssembly Version** - Run in browser
- [ ] **Collaborative Editing** - Make music with friends
- [ ] **Cloud Sync** - Optional project backup
- [ ] **Hardware Integration** - Better MIDI controller support
- [ ] **More AI Models** - Expanded generative tools
- [ ] **Score Editor** - Traditional notation view

### Long-Term Vision

- **AR/VR Support** - Sculpt sound in 3D space
- **Brain-Computer Interface** - Think your music
- **Quantum Audio Synthesis** - When quantum computers arrive
- **Universal Plugin Standard** - Cross-DAW compatibility

---

<p align="center">
  <strong>üéµ Music for Everyone üéµ</strong><br>
  <em>Made with ‚ù§Ô∏è by the Intuitives community</em>
</p>

<p align="center">
  <a href="https://github.com/mitchlabeetch/Intuitive_Music">GitHub</a> ‚Ä¢
  <a href="https://intuitivesdaw.com">Website</a> ‚Ä¢
  <a href="https://discord.gg/intuitives">Discord</a> ‚Ä¢
  <a href="https://twitter.com/IntuitivesDAW">Twitter</a>
</p>
